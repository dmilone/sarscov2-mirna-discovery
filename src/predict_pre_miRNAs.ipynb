{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "predict_pre-miRNAs.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn0GdEE1JpnN"
      },
      "source": [
        "# Using pre-miRNAs classifiers on SARS-CoV-2 genome\n",
        "This notebook have uses the train machine learning models to find new sarscov2 pre-miRNAs. It can easily run in a stand alone way with [Google Colaboratory](colab.research.google.com), otherwise a python instalation and a GPU are required.\n",
        "\n",
        "More details of the used models can be found in:\n",
        "\n",
        "- L. A. Bugnon, C. Yones, D. H. Milone, G. Stegmayer, Deep neural architectures for highly imbalanced data in bioinformatics, IEEE Transactions on Neural Networks and Learning Systems, 2019, https://doi.org/10.1109/TNNLS.2019.2914471\n",
        "- C. Yones, J. Raad, L.A. Bugnon, D.H. Milone, G. Stegmayer, High precision in microRNA prediction: a novel genome-wide approach based on convolutional deep residual networks, bioRxiv 2020.10.23.352179, 2020, https://doi.org/10.1101/2020.10.23.352179\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CKJu7W6oLf7K",
        "outputId": "211f048a-57f5-4f86-fb95-37b8056da75d"
      },
      "source": [
        "# Run this cell only if you are working from a google colab. This will download \n",
        "# the dataset and set the working directory.\n",
        "! sudo apt-get install git-lfs\n",
        "import os \n",
        "! git clone https://github.com/sinc-lab/sarscov2-mirna-discovery.git\n",
        "os.chdir(\"sarscov2-mirna-discovery/\")\n",
        "! git lfs pull # This download the large files (for example, the features)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  git-lfs\n",
            "0 upgraded, 1 newly installed, 0 to remove and 14 not upgraded.\n",
            "Need to get 2,129 kB of archives.\n",
            "After this operation, 7,662 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 git-lfs amd64 2.3.4-1 [2,129 kB]\n",
            "Fetched 2,129 kB in 0s (4,875 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 1.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package git-lfs.\n",
            "(Reading database ... 144865 files and directories currently installed.)\n",
            "Preparing to unpack .../git-lfs_2.3.4-1_amd64.deb ...\n",
            "Unpacking git-lfs (2.3.4-1) ...\n",
            "Setting up git-lfs (2.3.4-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Cloning into 'sarscov2-mirna-discovery'...\n",
            "remote: Enumerating objects: 40, done.\u001b[K\n",
            "remote: Counting objects: 100% (40/40), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 40 (delta 8), reused 36 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (40/40), done.\n",
            "Git LFS: (4 of 4 files) 799.86 MB / 799.86 MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aTHxcZKRWFh9"
      },
      "source": [
        "# Dataset preparation\n",
        "Here we load all the hairpin-like sequences found in the SARS-CoV2 genome and its features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NoYZE_Awnds"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import zscore  \n",
        "\n",
        "features_sarscov2 = pd.read_csv('features/sars-cov2_hairpins.csv')\n",
        "sequence_names = features_sarscov2.sequence_names.values\n",
        "features_sarscov2 = features_sarscov2.drop(columns=[\"sequence_names\"]).values.astype(float)\n",
        "\n",
        "# Feature normalization\n",
        "features_sarscov2[np.where(np.isnan(features_sarscov2))] = 0\n",
        "features_sarscov2 = zscore(features_sarscov2, axis=0)\n",
        "features_sarscov2[np.where(np.isnan(features_sarscov2))] = 0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NnmT8wQkKBMn"
      },
      "source": [
        "## One-Class SVM (OC-SVM)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC_Hmn5NVL5p"
      },
      "source": [
        "from sklearn.svm import OneClassSVM\n",
        "import pickle\n",
        "model_file = \"models/ocsvm.pk\"\n",
        "try:\n",
        "    ocsvm = pickle.load(open(model_file, \"rb\"))\n",
        "except FileNotFoundError:\n",
        "    raise f\"Model file {model_file} not found. You probably need to train the model first\"  \n",
        "\n",
        "if not os.path.isdir(\"predictions\"):\n",
        "    os.mkdir(\"predictions\")\n",
        "\n",
        "scores = ocsvm.decision_function(features_sarscov2) # better candidates at first\n",
        "ind = np.argsort(scores)[::-1]\n",
        "pd.DataFrame(np.array([sequence_names[ind], scores[ind]]).T, \n",
        "             columns=[\"sequence_names\", \"OC-SVM_scores\"]).to_csv(\"predictions/OC-SVM.csv\",\n",
        "                                                                 index=False)\n",
        "# If you are working in a google colab, you should see the output \"predictions\" \n",
        "# under the folder icon on the left panel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMx8lkloa9e9"
      },
      "source": [
        "# Deep Ensemble-Elastic Self-organized maps (deeSOM)\n",
        "You can find more details of the model implementation in the [deeSOM repository](https://github.com/lbugnon/deeSOM)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sKLGbM1_NoYd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cdeb23c-c04e-4e53-acfb-d98832d66216"
      },
      "source": [
        "!pip install deesom \n",
        "from deesom import DeeSOM\n",
        "deesom = DeeSOM(verbosity=True)\n",
        "model_file = \"models/deesom.pk\"\n",
        "try:\n",
        "    deesom.load_model(model_file)\n",
        "except FileNotFoundError:\n",
        "    raise f\"Model file {model_file} not found. You probably need to train the model first\"  \n",
        "\n",
        "if not os.path.isdir(\"predictions\"):\n",
        "    os.mkdir(\"predictions\")\n",
        "\n",
        "scores = deesom.predict_proba(features_sarscov2)\n",
        "ind = np.argsort(scores)[::-1]\n",
        "pd.DataFrame(np.array([sequence_names[ind], scores[ind]]).T, \n",
        "             columns=[\"sequence_names\", \"deeSOM_scores\"]).to_csv(\"predictions/deeSOM.csv\",\n",
        "                                                                 index=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: deesom in /usr/local/lib/python3.6/dist-packages (1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGMN4F-OKkK2"
      },
      "source": [
        "## miRNA Deep Neural Network (mirDNN)\n",
        "You can find more details of the model implementation in the [mirDNN repository](https://github.com/cyones/mirDNN)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3cNEFGDvKjjb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4ba99b02-3f37-4a0e-bc34-eed1e1eeac12"
      },
      "source": [
        "# Install mirDNN\n",
        "! git clone --recurse-submodules https://github.com/cyones/mirDNN.git\n",
        "! pip install -r mirDNN/requirements.txt\n",
        "\n",
        "! python mirDNN/model_eval.py -i \"sequences/sars-cov2_hairpins.fold\" -o \"predictions/mirDNN.csv\" -m models/mirDNN.pmt -s160 -d \"cuda\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: biopython in /usr/local/lib/python3.6/dist-packages (1.76)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from biopython) (1.18.2)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}